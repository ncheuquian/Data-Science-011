{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4373d6-342e-4456-a834-43b74a6bbf01",
   "metadata": {},
   "source": [
    "# tAPP 5: Obtaining, Cleaning and Validating Baby Names Data with Bash One-Liners, Part II\n",
    "\n",
    "**Author:** [Your Name]\n",
    "\n",
    "**Team:** [Your Team Name]\n",
    "\n",
    "**Date:** Mar 3, 2025\n",
    "\n",
    "**Course:** DSC 011 Sp25\n",
    "\n",
    "## The Goal and Task of This Assignment\n",
    "\n",
    "The Goal of today's Lecture is to continue to demonstrate using UNIX Bash command pipelines and Perl one-liners to validate and statistically explore the cleaned and integrated master data-file we created and started validating last time in tAPP 4 called `baby_clean.csv`, a small (2800 row by 6 column) data-set of top-200 U.S. baby names from the US Census data and downloaded from the Social Security Administration. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bfddc-1586-4728-ac1c-c29e857ef5c0",
   "metadata": {},
   "source": [
    "\n",
    "**Your Task: No matter whether you already worked tAPP4 and have the validated file `baby_clean.csv` or you missed tAPP4 and you do not have this file, run all of the code chunks with shift-enter in this notebook from top to bottom until you reach the section \"Answer a Fairly Hard Question About the Data.\" After that, wait at the section \"Answer a Fairly Hard Question About the Data.\" Then, follow the instructor's demonstration and enter your validated code as instructed. At the end of lecture, export your notebook to HTML and upload to the tAPP 5 assignment.**\n",
    "\n",
    "## Assignment \n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "### Personalize Your Notebook\n",
    "\n",
    "1. **In this notebook file, **please fill in your name and your team's name** and save the notebook.**\n",
    "2. **Rename your saved notebook with your own name.**\n",
    "\n",
    "### Validate Your Input Data \n",
    "\n",
    "3. **Run the code cells below in order from top to bottom.**\n",
    "\n",
    "Last time, we used a pretty vague shell glob to match our processed text input files with filenames like `names2000s.txt`. At some point we created some other files that could potentially match the shell glob and cause the bash interpreter to reduplicate some lines of data on the input. This is a form of data corruption, and we absolutely must not allow this to happen.  \n",
    "\n",
    "To see if this might be happening to you, let's remember that we have 14 decades of data. Let's use UNIX text utilities to test how many files we are matching with our shell globs. In the following command, note that the option to `ls` is `-1` the number 1, not `-l` the letter l. This prints each file on a separate line. The backticks evaluate the pipeline to its output value.\n",
    "\n",
    "4. **Run the following code cell with shift-enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e4557-0855-490d-b701-03e03643effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ `ls -1 names*.txt | wc -l` -eq 14 ]; then \n",
    "    echo \"CORRECT: 14 files matched with shell glob names*.txt\" \n",
    "elif [ `ls -1 names*.txt | wc -l` -gt 14 ]; then \n",
    "    echo \"INCORRECT: more than 14 files matched with shell glob names*.txt. Try a more specific shell glob.\"   \n",
    "else \n",
    "    echo \"INCORRECT: fewer than 14 files matched with shell glob names*.txt. You need to download or convert files.\"  \n",
    "fi;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790f0ff-dbc2-4941-96c9-e88a01d681ae",
   "metadata": {},
   "source": [
    "Let's try with a more specific shell glob that describes more exclusively our intended input file names for the decades from 1880s to 2010s. This can save the trouble of deleting files and let us proceed.\n",
    "\n",
    "5. **Then run the following code cell with shift-enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c008114-c629-4af5-86ce-4a73a50c83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ `ls -1 names[12][890][0-9]0s.txt | wc -l` -eq 14 ]; then \n",
    "    echo \"CORRECT: 14 files matched with shell glob names[12][890][0-9]0s.txt \" \n",
    "elif [ `ls -1 names[12][890][0-9]0s.txt | wc -l` -gt 14 ]; then \n",
    "    echo \"INCORRECT: more than 14 files matched with shell glob names[12][890][0-9]0s.txt. Something's wrong, you need to delete some files. Try running ls -1 names[12][90][0-9]0s.txt in a code cell to see what's going on.\"   \n",
    "else \n",
    "    echo \"INCORRECT: fewer than 14 files matched with shell glob names[12][890][0-9]0s.txt. You need to download or convert some of the files from the data source.\"  \n",
    "fi;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06f6fc-109e-4f13-b3e2-757519b22cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -1 names[12][890][0-9]0s.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325b224-189c-44b8-bb0d-1c9f974be04c",
   "metadata": {},
   "source": [
    "\n",
    "**The MORAL OF THE STORY with both shell globs and regular expressions is make them as specific and unambiguous as possible!**\n",
    "\n",
    "### Download and Convert Source Data (Only if Necessary!)\n",
    "\n",
    "**If the above two code chunks returned CORRECT, you may SKIP THIS SECTION to Step 7.**\n",
    "\n",
    "6. **Run the following code cell with shift-enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef20e6e-45df-4cc3-9505-b20fab36b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install html2text\n",
    "if [ `ls -1 names[12][890][0-9]0s.html | wc -l` -eq 14 ]; then \n",
    "    echo \"14 data source input files in html detected. Converting files to text in Python.\"\n",
    "    for decade in {188..201}0; do \n",
    "        html=\"names${decade}s.html\"; \n",
    "        text=\"names${decade}s.txt\"; \n",
    "        python -c 'import sys, html2text; print(html2text.html2text(sys.stdin.read()))' < $html > $text\n",
    "    done;\n",
    "else \n",
    "    echo \"Downloading source data and converting files to text in Python.\"\n",
    "    for decade in {188..201}0; do \n",
    "        html=\"names${decade}s.html\"; \n",
    "        text=\"names${decade}s.txt\"; \n",
    "        if [ -e $html ]; then\n",
    "            echo \"file $html exists. Skipping.\"\n",
    "        else\n",
    "            sleep 1; \n",
    "            wget --no-check-certificate \"https://www.ssa.gov/oact/babynames/decades/names${decade}s.html\"; \n",
    "            python -c 'import sys, html2text; print(html2text.html2text(sys.stdin.read()))' < $html > $text\n",
    "        fi;\n",
    "    done;\n",
    "fi;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2ea71-6c0a-405c-8897-8fbc477c1785",
   "metadata": {},
   "source": [
    "You may now re-run the tests under **Validate Your Input Data** above to validate that it shows \"CORRECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59622a5-5749-45fe-9321-34ed4495a316",
   "metadata": {},
   "source": [
    "### Extract Data Lines with Extended Bash Regular Expressions and Validate the Extraction (Only if Necessary!)\n",
    "\n",
    "Last time we demonstrated the following POSIX-compliant extended regular expression in Bash `egrep` to match all of the 200 data lines from each of the 14 processed input data text files. We'll use `head` to just sample the first ten lines. Notice that when using `grep` or `egrep` with a shell glob, by default the output is labelled with file of origin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab60dd7-9ca3-4117-aa33-9cb90c073aac",
   "metadata": {},
   "source": [
    "Let's run some tests to validate that we are all extracting exactly the same number of lines and the same data.\n",
    "\n",
    "7. **Run the following code cell with shift-enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc1a2-1ca8-4b54-95ae-57bd8dd7e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"testing number of lines...\"\n",
    "if [ `grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | wc -l` -eq 2800 ]; then \n",
    "    echo \"2800 data lines detected. CORRECT.\"\n",
    "else \n",
    "    echo \"Fewer or Greater than 2800 data lines detected. INCORRECT. Amoungt found was:\"\n",
    "    grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | wc -l\n",
    "    echo \"Investigate the output interactively in the cell below using wc, egrep, sort, cut, etc\"\n",
    "fi;\n",
    "echo \"testing equality of data...\"\n",
    "echo \"...first ten lines...\"\n",
    "grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | head\n",
    "echo \"...md5sum...\"\n",
    "grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | head | md5sum\n",
    "if [[ `grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | head | md5sum` == \"ddba814d0a3396d4bf85e9d59e6ada2b  -\" ]]; then \n",
    "    echo \"md5sum of first ten lines is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of first ten lines is INCORRECT.\"\n",
    "fi;\n",
    "echo \"...all data...\"\n",
    "echo \"...md5sum...\"\n",
    "grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | md5sum\n",
    "if [[ `grep -E \"^[0-9]+ \\|\" names[12][890][0-9]0s.txt | md5sum` == \"f110abd32f52d122b81119d08bf0c3ee  -\" ]]; then \n",
    "    echo \"md5sum of all data is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of all data is INCORRECT.\"\n",
    "fi;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f713993-88d9-4bde-a781-53cf1861ebc8",
   "metadata": {},
   "source": [
    "### Extract Data Lines with Perl Regular Expressions and Validate the Extraction\n",
    "\n",
    "We'll see that `perl` is a powerful extension of the UNIX tools, replacing and superceding what `awk` and `sed` can do alone. Perl offers more powerful regular expression dialect too. In perl we can use `\\d` instead of `[0-9]`. We can also use `\\s` for white-space in general, matching both tabs and space charachters:\n",
    "\n",
    "8. **Run the following code cell with shift-enter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475e0c8-f244-48d6-a14a-9cd4ccdfe83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"testing number of lines with perl regexes using perl...\"\n",
    "if [ `perl -ne 'print if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt | wc -l` -eq 2800 ]; then \n",
    "    echo \"2800 data lines detected. CORRECT.\"\n",
    "else \n",
    "    echo \"Fewer or Greater than 2800 data lines detected with perl. INCORRECT. Amoungt found was:\"\n",
    "    perl -ne 'print if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt | wc -l\n",
    "    echo \"Investigate the output interactively in the cell below using wc, egrep, sort, cut, etc\"\n",
    "fi;\n",
    "echo \"testing equality of data...\"\n",
    "echo \"...first ten lines...\"\n",
    "perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt| head\n",
    "echo \"...md5sum...\"\n",
    "perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt| head | md5sum\n",
    "if [[ `perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt| head | md5sum` == \"ddba814d0a3396d4bf85e9d59e6ada2b  -\" ]]; then \n",
    "    echo \"md5sum of first ten lines is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of first ten lines is INCORRECT.\"\n",
    "fi;\n",
    "echo \"...all data...\"\n",
    "echo \"...md5sum...\"\n",
    "perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt| md5sum\n",
    "if [[ `perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt| md5sum` == \"f110abd32f52d122b81119d08bf0c3ee  -\" ]]; then \n",
    "    echo \"md5sum of all data is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of all data is INCORRECT.\"\n",
    "fi;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c6840-9650-4f34-8f0c-c80a8dcdb3c2",
   "metadata": {},
   "source": [
    "## Clean Baby Names Data, Convert to CSV, and Validate (Only if Necessary!)\n",
    "\n",
    "7. **Follow Instructor Deminstratino to Extract and Clean Data by editing the top of the code cell with a bash one-liner using a perl anonymous program for extraction.**\n",
    "8. **Re-Run the following code cell with shift-enter to validate the one-liner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3e0d7-eb99-4d94-acde-4e620ad3109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perl -ne 'print \"$ARGV:$_\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt | cut -c 6-10,15- | tr -d \" ,\" | tr \":|\" , > baby_clean.csv\n",
    "if [ `cat baby_clean.csv | wc -l` -eq 2800 ]; then \n",
    "    echo \"2800 data lines detected in baby_clean.csv. CORRECT.\"\n",
    "else \n",
    "    echo \"Fewer or Greater than 2800 data lines detected in baby_clean.csv. INCORRECT. Amoungt found was:\"\n",
    "    cat baby_clean.csv | wc -l\n",
    "    echo \"Investigate the output interactively in the cell below using wc, egrep, sort, cut, etc\"\n",
    "fi;\n",
    "echo \"testing equality of data...\"\n",
    "echo \"...first ten lines...\"\n",
    "head baby_clean.csv\n",
    "echo \"...md5sum...\"\n",
    "head baby_clean.csv | md5sum\n",
    "if [[ `head baby_clean.csv | md5sum` == \"6d8913fae285edec358d107db11dc4ed  -\" ]]; then \n",
    "    echo \"md5sum of first ten lines of baby_clean.csv is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of first ten lines of baby_clean.csv is INCORRECT.\"\n",
    "fi;\n",
    "echo \"...all data...\"\n",
    "echo \"...md5sum...\"\n",
    "cat baby_clean.csv | md5sum\n",
    "if [[ `cat baby_clean.csv | md5sum` == \"8fee16f6cd371c349dc6f30c04e4648c  -\" ]]; then \n",
    "    echo \"md5sum of all data in baby_clean.csv is CORRECT.\"\n",
    "else \n",
    "    echo \"ERROR: md5sum of all data in baby_clean.csv INCORRECT.\"\n",
    "fi;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f68d2ae-fb1d-4dac-8753-6449b87a076a",
   "metadata": {},
   "source": [
    "## Answer a Fairly Hard Question About the Data \n",
    "\n",
    "Write a single UNIX pipeline to answer the question (as an output number included in the notebook output you turn in): **how many unique boys and girls names are included in this dataset from all fourteen decades?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b47f8b-b920-4eb2-bf01-24776adf9d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique boy's and girl's names over all 2800 names in the top U.S. 200 most popular names by decade from 1880s to 2010s:\n"
     ]
    }
   ],
   "source": [
    "echo \"Number of unique boy's and girl's names over all 2800 names in the top U.S. 200 most popular names by decade from 1880s to 2010s:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b87d4-2a7c-4f2b-a9db-9366dce44672",
   "metadata": {},
   "source": [
    "## Answer an Even Harder Question About the Data \n",
    "\n",
    "Write a single UNIX command pipeline to answer the question (as an output number included in the notebook output you turn in): **what is the frequency distribution of first letters among boy's names sorted by frequency from most common to least common?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0f9fd-be22-499b-a9c7-dcbfda1dc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Sorted Frequency Distribution of First Letters Among Top-Ranked Boy's Names over all 14 decades:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8a405-eae7-464d-8e00-3d68e5beec3f",
   "metadata": {},
   "source": [
    "## Use Python f-strings to Format the Number of Unique Names as a Percentage of the Total Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c03efc-3491-407d-a313-0e946d647b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File \"<string>\", line 1\n",
      "    (100 * ( / ))\n",
      "             ^\n",
      "SyntaxError: f-string: invalid syntax\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "python -c 'print(f\"{100 * ( / ):.2f}%\")'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c67aa-8014-4fdf-b0a9-e8107f33aa80",
   "metadata": {},
   "source": [
    "## Use Perl to Answer a Question that Cannot Be Answered with UNIX Text Utilities\n",
    "\n",
    "While this first-letter frequency distribution over names is interesting, it doesn't predict well what a person who lived over these 14 decades would experiences as the first-letter distribution of first name's, because their actual experience of letters would be influenced by the **numbers of babies** born with each name. In order to compute this frequency distribution, we need to add up the numbers of babies born with each name. This is hard to do with UNIX text utilities because they don't treat strings of digits as numbers. Here is where Perl comes to the rescue.\n",
    "\n",
    "**What is the frequency distribution of first letters among boy's names weighted by numbers of babies born with that name, sorted by frequencies per baby from most common to least common?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305deba-aec7-45b4-9856-d1869e8b61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"per baby:\"\n",
    "cut -d, -f baby_clean.csv | tr  | sort | perl -F, -ane '$ += $;END{foreach my $ (sort keys %){print \"\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1644f2-4f6b-421f-900b-f7927bdfaa9f",
   "metadata": {},
   "source": [
    "## Use Perl to Check that the Number of Fields is the Same Over All Rows of Data\n",
    "\n",
    "**How many fields are on every line of baby_clean.csv?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a4391-fd37-407c-adf7-f113faf9b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b53a6c5c-0500-4aae-a4a8-4c521181155a",
   "metadata": {},
   "source": [
    "## Use Perl to Validate a Calculated Frequency Distribution\n",
    "\n",
    "**What is the sum of first-letter frequencies of boy's names, is it equal to the total number of boy's names in the data?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa8476-9dc9-4bbd-bcec-64dcae8af86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"sum of frequencies per name:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c50df-0ef7-4300-965b-6a723df159ad",
   "metadata": {},
   "source": [
    "**What is the sum of first-letter frequencies of boy's names weighted by numbers of births, is it equal to the total number of male babies in the data?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd60b4-3f78-45f8-ac68-1d7dcc77010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"total male babies or sum of frequencies per baby:\"\n",
    "echo \"total male babies in original source data:\"\n",
    "perl -ane 'print \"\" if (/^\\d+\\s\\|/)' names[12][890][0-9]0s.txt | tr -d , | perl -ne ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cccf7-e3e2-4e89-a106-ded44a552f84",
   "metadata": {},
   "source": [
    "**Please export your worked notebook containing command output to HTML and upload it to CatCourses at the end of Lecture.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
